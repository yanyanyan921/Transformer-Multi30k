from torchtext.legacy.data import Field, BucketIterator
from torchtext.legacy.datasets.translation import Multi30k
import torchtext
import spacy
import os
import ssl

'''
è¯·ç¡®ä¿å·²å®‰è£… spacy åŠå¯¹åº”çš„è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚ï¼š
    pip install spacy
    python -m spacy download en_core_web_sm
    python -m spacy download de_core_news_smï¼ˆè¿™ä¸ªæ›´å¥½ï¼šconda install -c conda-forge spacy-model-de_core_news_smï¼‰
    ç”¨condaä¸‹è½½æ›´å¥½ï¼Œä¸€å®šè¦æ³¨æ„ç‰ˆæœ¬ï¼›GPUç”¨3090ä¸ç„¶ä¼šç‰ˆæœ¬å†²çª
    æœåŠ¡å™¨ä¸Šæœ€å¥½æœ¬åœ°ä¸‹è½½,å†ä¸Šä¼ åˆ°æœåŠ¡å™¨ï¼Œå†å®‰è£…
    è‹±è¯­æ¨¡å‹ï¼šhttps://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl
    å¾·è¯­æ¨¡å‹ï¼šhttps://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl
è¾“å…¥è¾“å‡ºè¯´æ˜ï¼š
--------------
è¾“å…¥ï¼š
    - text: åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²
    - lang: è¯­è¨€ä»£ç ï¼ˆå¦‚ "en", "zh", "fr"ï¼‰
è¾“å‡ºï¼š
    - token_list: åˆ†è¯ç»“æœï¼ˆList[str]ï¼‰
'''
class Tokenizer:
    def __init__(self):
        """
        åˆå§‹åŒ– tokenizer,åŠ è½½æ‰€éœ€çš„spacyæ¨¡å‹
        """
        try:
            self.spacy_de = spacy.load('de_core_news_sm')
        except OSError:
            raise RuntimeError("è¯·å…ˆè¿è¡Œï¼š'python -m spacy download de_core_news_sm")

        try:
            self.spacy_en = spacy.load('en_core_web_sm')
        except OSError:
            raise RuntimeError("è¯·å…ˆè¿è¡Œï¼š'python -m spacy download en_core_web_sm")

    def tokenize_de(self, text: str) -> list:
        return [tok.text for tok in self.spacy_de.tokenizer(text)]

    def tokenize_en(self, text: str) -> list:
        return [tok.text for tok in self.spacy_en.tokenizer(text)]


class Dataset:
    source: Field = None
    target: Field = None
    def __init__(self, ext, tokenize_en, tokenize_de, init_token, eos_token):
        self.ext = ext  # æ‰©å±•åï¼ˆå¦‚ ".de" æˆ– ".en"ï¼‰
        self.tokenize_en = tokenize_en  #è‹±è¯­åˆ†è¯å‡½æ•°
        self.tokenize_de = tokenize_de  #å¾·è¯­åˆ†è¯å‡½æ•°
        self.init_token = init_token  #åˆå§‹åŒ– tokenï¼ˆå¦‚ "<sos>"ï¼‰
        self.eos_token = eos_token  #ç»“æŸ tokenï¼ˆå¦‚ "<eos>"ï¼‰
        print(f"æ•°æ®é›†å¼€å§‹åˆå§‹åŒ–...")
        current_dir = os.path.dirname(os.path.abspath(__file__))  #Dataloader.pyçš„çˆ¶ç›®å½•ï¼Œå³dataåŒ…
        self.root_path = os.path.join(current_dir, '.data')  # è¿™æ ·å¾—åˆ°çš„æ˜¯ data/.data

    def make_dataset(self):
        if self.ext == ('.de', '.en'):
            self.source = Field(tokenize=self.tokenize_de, init_token=self.init_token, eos_token=self.eos_token,
                        lower=True, batch_first=True)
            self.target = Field(tokenize=self.tokenize_en, init_token=self.init_token, eos_token=self.eos_token,
                        lower=True, batch_first=True)

        elif self.ext == ('.en', '.de'):
            self.source = Field(tokenize=self.tokenize_en, init_token=self.init_token, eos_token=self.eos_token,
                        lower=True, batch_first=True)
            self.target = Field(tokenize=self.tokenize_de, init_token=self.init_token, eos_token=self.eos_token,
                        lower=True, batch_first=True)
        #æ•°æ®å¾ˆéš¾ä¸‹è½½ï¼Œæœ‰æœ¬åœ°æ–‡ä»¶ä»–å°±ä¸ä¼šä¸‹è½½äº†ç›´æ¥ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        # ç¦ç”¨ä¸‹è½½ï¼Œç›´æ¥ä½¿ç”¨æœ¬åœ°æ–‡ä»¶

        try:
            train_data, valid_data, test_data = Multi30k.splits(
                exts=self.ext,
                fields=(self.source, self.target),
                root=self.root_path
            )
            print("æœ¬åœ°æ•°æ®åŠ è½½æˆåŠŸï¼")
            return train_data, valid_data, test_data
        except Exception as e:
            raise RuntimeError(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")

    def build_vocab(self, train_data, min_freq):
        """
        æ„å»ºè¯æ±‡è¡¨
        train_data: torchtext.datasets.TranslationDataset
        min_freq (int): æœ€å°è¯é¢‘ï¼ˆé»˜è®¤ä¸º2ï¼‰
        """
        self.source.build_vocab(train_data, min_freq=min_freq)
        self.target.build_vocab(train_data, min_freq=min_freq)

    def make_iter(self, train, validate, test, batch_size, device):
        """
         åˆ›å»ºè¿­ä»£å™¨
         train: torchtext.datasets.TranslationDataset
         validate: torchtext.datasets.TranslationDataset
         test: torchtext.datasets.TranslationDataset
         """
        train_iter, valid_iter, test_iter = BucketIterator.splits(
            (train, validate, test),
            batch_size=batch_size,
            device=device)
        print(f"æ•°æ®é›†åˆå§‹åŒ–ç»“æŸ...")
        return train_iter, valid_iter, test_iter

if __name__ == "__main__":
    print("ğŸ’¬ Tokenizer demo:")
    tokenizer =Tokenizer()
    de_sentence = "Ich liebe natÃ¼rliche Sprachverarbeitung."
    en_sentence = "I love natural language processing."
    print("Input:", de_sentence)
    print("Tokens:", tokenizer.tokenize_de(de_sentence))
    print("Input:", en_sentence)
    print("Tokens:", tokenizer.tokenize_en(en_sentence))
    print("Tokenizer demo finished.")